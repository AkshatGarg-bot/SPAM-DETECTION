{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from text_unidecode import unidecode\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, ENGLISH_STOP_WORDS, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn import  metrics\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "\n",
    "def text_preprocessing(df, lis):\n",
    "    text_data = df[\"MainText\"]\n",
    "    \n",
    "    punct = set(string.punctuation)\n",
    "    stopwords = set(ENGLISH_STOP_WORDS)\n",
    "    lm = WordNetLemmatizer()\n",
    "    \n",
    "    for i in range(len(text_data)):\n",
    "        line = text_data[i]\n",
    "        \n",
    "        line = line.strip()\n",
    "        line = \" \".join(line.split())\n",
    "\n",
    "        line = unidecode(line)\n",
    "        \n",
    "        line = line.lower()\n",
    "        line = ''.join(char for char in line if char not in punct)\n",
    "        tokens = re.split('\\W+', line)\n",
    "        \n",
    "        line = [lm.lemmatize(word) for word in tokens if word not in stopwords]\n",
    "        \n",
    "        lis.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"our_contest_train_2.csv\")\n",
    "primary = df[[\"PrimarySubject\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=[\"MainText\"], inplace=True)\n",
    "df = df[[\"MainText\", \"Target\"]]\n",
    "tf= df.copy()\n",
    "del df\n",
    "df = tf.copy()\n",
    "del tf\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis = []\n",
    "text_preprocessing(df, lis)\n",
    "y = df[[\"Target\"]]\n",
    "del df\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "sentence = [''.join(o) for o in lis]\n",
    "del lis\n",
    "vec_counter = vectorizer.fit_transform(sentence)\n",
    "del sentence\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(vec_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
    "clf = clf.fit(df, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_tfidf\n",
    "df_test = pd.read_csv(\"our_contest_test.csv\")\n",
    "df_test = df_test[[\"MainText\"]]\n",
    "primary = df_test[[\"PrimarySubject\"]]\n",
    "tf = df_test.copy()\n",
    "del df_test\n",
    "df_test= tf.copy()\n",
    "del tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis = []\n",
    "text_preprocessing(df_test, lis)\n",
    "primary = df_test[[\"PrimarySubject\"]]\n",
    "\n",
    "del df_test\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "sentence = [''.join(o) for o in lis]\n",
    "del lis\n",
    "vec_counter = vectorizer.fit_transform(sentence)\n",
    "del sentence\n",
    "X_test_tfidf = tfidf_transformer.fit_transform(vec_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test_tfidf)\n",
    "del X_test_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_read = pd.read_csv('./our_contest_sample_solution_2.csv')\n",
    "y_pred = pd.DataFrame(y_pred, columns=[\"Target\"])\n",
    "\n",
    "dummy_read['Target'] = y_pred\n",
    "dummy_read['Target'] = dummy_read['Target'].astype(int)\n",
    "\n",
    "dummy_read.to_csv(\"XGBoost.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[\"Target\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_read = pd.read_csv('./our_contest_sample_solution_2.csv')\n",
    "y_pred = pd.DataFrame(y_pred, columns=[\"Target\"])\n",
    "\n",
    "dummy_read['Target'] = y_pred\n",
    "dummy_read['Target'] = dummy_read['Target'].astype(int)\n",
    "\n",
    "dummy_read.to_csv(\"Lightgbm.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0391686cbb9b72fa09d457be7b8e1df2b4428c048dfa8cfb00e45794f739311f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
